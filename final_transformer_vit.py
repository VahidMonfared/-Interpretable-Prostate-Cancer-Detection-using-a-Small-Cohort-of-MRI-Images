# -*- coding: utf-8 -*-
"""Final Transformer ViT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j1xPzDGLy-2wAeANva1A6iiQpZvNkdpi
"""

#!rm -rf /content/*

"""Transformer ViT"""

# ============================================================
# Prostate T2 MRI Cancer Detection (Colab - One Snippet)
# TRANSFORMER VERSION (ViT-B/16 transfer learning)
# - ViT transfer learning (pretrained)
# - Robust preprocessing (resize/normalize grayscale->3ch)
# - Augmentation (rotation/affine/flip)
# - Class imbalance (WeightedRandomSampler + pos_weight)
# - 80/20 split (train+val via K-fold CV on 80%, separate 20% held-out test)
# - Long early stopping (patience), epochs=100
# - Metrics for Train/Val/Test: Acc, Prec, Recall(Sens), Spec, F1, AUC, TN/FP/FN/TP
# - ROC curves, confusion matrices, train-vs-val curves
# - Transformer Explainability: Attention Rollout (Grad-CAM equivalent for ViT)
#   4x4 grid (16 images) = top-8 high-prob cancer + top-8 high-prob normal (from TEST)
# - Risk stratification (Low/Inter/High), embeddings + UMAP + outlier/“suspicious normals”
# - Save model (.pt) + pickle (.pkl) for web app
# ============================================================

!pip -q install scikit-learn matplotlib opencv-python umap-learn timm

import os, zipfile, random, math, pickle
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from PIL import Image

from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, roc_auc_score, roc_curve
)

import umap
from sklearn.cluster import KMeans

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, Subset, WeightedRandomSampler
from torchvision import transforms

import timm

# ----------------------------
# Reproducibility
# ----------------------------
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

# ----------------------------
# Paths (your zips)
# ----------------------------
CANCER_ZIP = "/content/Cancer.zip"
NORMAL_ZIP = "/content/Normal.zip"
WORKDIR = "/content/data_prostate_vit"
os.makedirs(WORKDIR, exist_ok=True)

def unzip_to_dir(zip_path, out_dir):
    with zipfile.ZipFile(zip_path, 'r') as z:
        z.extractall(out_dir)

unzip_to_dir(CANCER_ZIP, WORKDIR)
unzip_to_dir(NORMAL_ZIP, WORKDIR)

# Expect folder names Cancer and Normal somewhere under WORKDIR.
# If your structure is different, this still scans recursively.
def collect_images(root):
    exts = (".png",".jpg",".jpeg")
    files=[]
    for r,_,fs in os.walk(root):
        for f in fs:
            if f.lower().endswith(exts):
                files.append(os.path.join(r,f))
    return sorted(files)

cancer_dir = os.path.join(WORKDIR, "Cancer")
normal_dir = os.path.join(WORKDIR, "Normal")

if os.path.isdir(cancer_dir) and os.path.isdir(normal_dir):
    cancer_files = collect_images(cancer_dir)
    normal_files = collect_images(normal_dir)
else:
    all_files = collect_images(WORKDIR)
    cancer_files = [p for p in all_files if "cancer" in p.lower()]
    normal_files = [p for p in all_files if "normal" in p.lower()]

print("Cancer images:", len(cancer_files))
print("Normal images:", len(normal_files))

assert len(cancer_files) > 0 and len(normal_files) > 0, "Could not find Cancer/Normal images after unzip."

X = cancer_files + normal_files
y = np.array([1]*len(cancer_files) + [0]*len(normal_files), dtype=int)

# ----------------------------
# Train/Test split: 80/20
# ----------------------------
X_trainval, X_test, y_trainval, y_test = train_test_split(
    X, y, test_size=0.20, random_state=SEED, stratify=y
)
print("\nSplit sizes:")
print("Train+Val:", len(X_trainval), "Test:", len(X_test))
print("Train+Val cancer:", y_trainval.sum(), "normal:", len(y_trainval)-y_trainval.sum())
print("Test cancer:", y_test.sum(), "normal:", len(y_test)-y_test.sum())

# ----------------------------
# Dataset + Transforms
# - images are grayscale; convert to 3 channels for ViT
# - resize to fixed size
# ----------------------------
IMG_SIZE = 224

tf_train = transforms.Compose([
    transforms.Grayscale(num_output_channels=3),
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.RandomRotation(10),
    transforms.RandomAffine(degrees=0, translate=(0.03,0.03), scale=(0.95,1.05)),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.ToTensor(),
    transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5]),
])

tf_eval = transforms.Compose([
    transforms.Grayscale(num_output_channels=3),
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5]),
])

class ProstateDataset(Dataset):
    def __init__(self, paths, labels, transform):
        self.paths = list(paths)
        self.labels = np.array(labels, dtype=int)
        self.transform = transform
    def __len__(self):
        return len(self.paths)
    def __getitem__(self, idx):
        p = self.paths[idx]
        img = Image.open(p).convert("L")
        x = self.transform(img)
        y = torch.tensor(self.labels[idx], dtype=torch.float32)
        return x, y, p

# Global test dataset/loader (fixed)
ds_test = ProstateDataset(X_test, y_test, tf_eval)

# ----------------------------
# Model: ViT transfer learning
# ----------------------------
class ViTBinary(nn.Module):
    def __init__(self, backbone_name="vit_base_patch16_224", dropout=0.35, freeze_backbone=False):
        super().__init__()
        self.backbone = timm.create_model(backbone_name, pretrained=True, num_classes=0)  # returns embedding
        if freeze_backbone:
            for p in self.backbone.parameters():
                p.requires_grad = False
        self.embed_dim = self.backbone.num_features
        self.head = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(self.embed_dim, 1)
        )
    def forward(self, x):
        emb = self.backbone(x)            # (B, D)
        logits = self.head(emb)           # (B, 1)
        return logits, emb

def build_vit(dropout=0.35, freeze_backbone=False):
    m = ViTBinary(dropout=dropout, freeze_backbone=freeze_backbone)
    return m.to(device)

# ----------------------------
# Metrics helpers
# ----------------------------
def compute_metrics(y_true, y_prob, thr=0.5):
    y_true = np.array(y_true).astype(int)
    y_prob = np.array(y_prob).astype(float)
    y_pred = (y_prob >= thr).astype(int)

    cm = confusion_matrix(y_true, y_pred, labels=[0,1])
    tn, fp, fn, tp = cm.ravel()

    acc = accuracy_score(y_true, y_pred)
    prec = precision_score(y_true, y_pred, zero_division=0)
    rec = recall_score(y_true, y_pred, zero_division=0)   # sensitivity
    f1 = f1_score(y_true, y_pred, zero_division=0)
    spec = tn / (tn + fp + 1e-12)

    auc = roc_auc_score(y_true, y_prob) if len(np.unique(y_true)) > 1 else float("nan")

    return {
        "acc": acc, "prec": prec, "recall": rec, "spec": spec, "f1": f1, "auc": auc,
        "tn": tn, "fp": fp, "fn": fn, "tp": tp, "cm": cm
    }

def plot_confusion(cm, title):
    plt.figure(figsize=(4.3,3.7))
    plt.imshow(cm)
    plt.title(title)
    plt.colorbar()
    plt.xticks([0,1], ["Normal","Cancer"])
    plt.yticks([0,1], ["Normal","Cancer"])
    for i in range(2):
        for j in range(2):
            plt.text(j,i,str(cm[i,j]),ha="center",va="center")
    plt.xlabel("Pred"); plt.ylabel("True")
    plt.tight_layout(); plt.show()

def plot_roc_curve(y_true, y_prob, title):
    fpr, tpr, _ = roc_curve(y_true, y_prob)
    auc = roc_auc_score(y_true, y_prob) if len(np.unique(y_true))>1 else float("nan")
    plt.figure(figsize=(4.5,3.8))
    plt.plot(fpr, tpr, label=f"AUC={auc:.3f}")
    plt.plot([0,1], [0,1], "--")
    plt.title(title); plt.xlabel("FPR"); plt.ylabel("TPR")
    plt.legend(); plt.tight_layout(); plt.show()

def plot_train_val_curves(history, title):
    ep = np.arange(1, len(history["train_loss"])+1)
    plt.figure(figsize=(6,4))
    plt.plot(ep, history["train_loss"], label="Train loss")
    plt.plot(ep, history["val_loss"], label="Val loss")
    plt.title(title+" - Loss"); plt.xlabel("Epoch"); plt.ylabel("Loss")
    plt.legend(); plt.tight_layout(); plt.show()

    plt.figure(figsize=(6,4))
    plt.plot(ep, history["train_acc"], label="Train acc")
    plt.plot(ep, history["val_acc"], label="Val acc")
    plt.title(title+" - Accuracy"); plt.xlabel("Epoch"); plt.ylabel("Accuracy")
    plt.legend(); plt.tight_layout(); plt.show()

# ----------------------------
# Training/Eval loops
# ----------------------------
@torch.no_grad()
def predict_probs_and_embeddings(model, loader):
    model.eval()
    probs = []
    ys = []
    paths = []
    embs = []
    for x,y,p in loader:
        x = x.to(device)
        logits, emb = model(x)
        pr = torch.sigmoid(logits).detach().cpu().numpy().ravel()
        probs.extend(pr.tolist())
        ys.extend(y.numpy().astype(int).ravel().tolist())
        paths.extend(list(p))
        embs.append(emb.detach().cpu().numpy())
    embs = np.vstack(embs) if len(embs) else np.zeros((0,1))
    return np.array(ys), np.array(probs), paths, embs

def make_weighted_sampler(y_labels):
    y_labels = np.array(y_labels, dtype=int)
    counts = np.bincount(y_labels, minlength=2)
    class_w = 1.0 / (counts + 1e-12)
    sample_w = class_w[y_labels]
    sampler = WeightedRandomSampler(sample_w, num_samples=len(sample_w), replacement=True)
    return sampler, counts

def train_one_fold(
    fold_id, train_idx, val_idx, X_tv, y_tv,
    epochs=100, batch_size=16, lr=3e-5, wd=1e-3,
    dropout=0.35, patience=25, min_delta=1e-4
):
    # Datasets per fold
    X_tr = [X_tv[i] for i in train_idx]
    y_tr = [y_tv[i] for i in train_idx]
    X_va = [X_tv[i] for i in val_idx]
    y_va = [y_tv[i] for i in val_idx]

    ds_tr = ProstateDataset(X_tr, y_tr, tf_train)
    ds_va = ProstateDataset(X_va, y_va, tf_eval)

    sampler, counts = make_weighted_sampler(y_tr)
    pos_weight = torch.tensor([counts[0]/max(counts[1],1)], dtype=torch.float32).to(device)

    dl_tr = DataLoader(ds_tr, batch_size=batch_size, sampler=sampler, num_workers=2, pin_memory=True)
    dl_va = DataLoader(ds_va, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)

    # Strategy to reduce overfit:
    # 1) Freeze backbone for first 5 epochs, train head only
    # 2) Then unfreeze and fine-tune with low LR
    model = build_vit(dropout=dropout, freeze_backbone=True)

    crit = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
    opt = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=wd)

    history = {"train_loss":[], "val_loss":[], "train_acc":[], "val_acc":[]}

    best_auc = -1
    best_state = None
    best_epoch = 0
    bad = 0

    print(f"\n========== Fold {fold_id} ==========")
    print("Train counts [normal,cancer]:", counts, "pos_weight:", float(pos_weight.item()))

    for ep in range(1, epochs+1):

        # Unfreeze after warmup (reduces overfitting spikes)
        if ep == 6:
            for p in model.backbone.parameters():
                p.requires_grad = True
            opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)

        # ---- TRAIN ----
        model.train()
        tr_losses=[]
        tr_probs=[]
        tr_true=[]

        for x,y,_ in dl_tr:
            x = x.to(device)
            y = y.to(device).unsqueeze(1)

            opt.zero_grad()
            logits, _ = model(x)
            loss = crit(logits, y)
            loss.backward()
            opt.step()

            tr_losses.append(loss.item())
            tr_probs.extend(torch.sigmoid(logits).detach().cpu().numpy().ravel().tolist())
            tr_true.extend(y.detach().cpu().numpy().ravel().astype(int).tolist())

        # ---- VAL ----
        model.eval()
        va_losses=[]
        va_probs=[]
        va_true=[]
        with torch.no_grad():
            for x,y,_ in dl_va:
                x = x.to(device)
                y = y.to(device).unsqueeze(1)
                logits, _ = model(x)
                loss = crit(logits, y)
                va_losses.append(loss.item())
                va_probs.extend(torch.sigmoid(logits).cpu().numpy().ravel().tolist())
                va_true.extend(y.cpu().numpy().ravel().astype(int).tolist())

        tr_m = compute_metrics(tr_true, tr_probs, thr=0.5)
        va_m = compute_metrics(va_true, va_probs, thr=0.5)

        history["train_loss"].append(float(np.mean(tr_losses)))
        history["val_loss"].append(float(np.mean(va_losses)))
        history["train_acc"].append(tr_m["acc"])
        history["val_acc"].append(va_m["acc"])

        # EXACT log format you requested
        print(f"Ep {ep:03d} | "
              f"Tr loss {np.mean(tr_losses):.4f} acc {tr_m['acc']:.3f} auc {tr_m['auc']:.3f} | "
              f"Va loss {np.mean(va_losses):.4f} acc {va_m['acc']:.3f} auc {va_m['auc']:.3f}")

        # Early stopping on VAL AUC (long patience)
        improved = (va_m["auc"] > best_auc + min_delta)
        if improved:
            best_auc = va_m["auc"]
            best_state = {k:v.detach().cpu().clone() for k,v in model.state_dict().items()}
            best_epoch = ep
            bad = 0
        else:
            bad += 1

        if bad >= patience:
            print(f"Early stopping triggered at epoch {ep} (best epoch {best_epoch}, best val AUC {best_auc:.4f})")
            break

    # restore best
    model.load_state_dict(best_state)
    return model, history, (ds_tr, ds_va), best_auc

# ----------------------------
# Cross-validation on Train+Val (80%)
# ----------------------------
K = 5  # 5-fold typical for small medical datasets
skf = StratifiedKFold(n_splits=K, shuffle=True, random_state=SEED)

fold_models = []
fold_histories = []
fold_val_aucs = []

X_tv = list(X_trainval)
y_tv = np.array(y_trainval, dtype=int)

for fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_tv, y_tv), start=1):
    model, history, (ds_tr, ds_va), best_auc = train_one_fold(
        fold_id, tr_idx, va_idx, X_tv, y_tv,
        epochs=100, batch_size=16, lr=3e-5, wd=1e-3,
        dropout=0.35, patience=25, min_delta=1e-4
    )
    fold_models.append((model, ds_tr, ds_va))
    fold_histories.append(history)
    fold_val_aucs.append(best_auc)

print("\nCV val AUCs:", [float(x) for x in fold_val_aucs])
best_fold = int(np.argmax(fold_val_aucs))
print("Best fold:", best_fold+1, "Best val AUC:", float(fold_val_aucs[best_fold]))

# Plot curves for best fold
plot_train_val_curves(fold_histories[best_fold], f"Best Fold {best_fold+1} Train vs Val")

best_model, best_ds_tr, best_ds_va = fold_models[best_fold]

# ----------------------------
# Final evaluation: Train / Val (from best fold) / Test
# ----------------------------
dl_tr_best = DataLoader(best_ds_tr, batch_size=16, shuffle=False, num_workers=2, pin_memory=True)
dl_va_best = DataLoader(best_ds_va, batch_size=16, shuffle=False, num_workers=2, pin_memory=True)
dl_test = DataLoader(ds_test, batch_size=16, shuffle=False, num_workers=2, pin_memory=True)

y_tr_true, y_tr_prob, tr_paths, tr_emb = predict_probs_and_embeddings(best_model, dl_tr_best)
y_va_true, y_va_prob, va_paths, va_emb = predict_probs_and_embeddings(best_model, dl_va_best)
y_te_true, y_te_prob, te_paths, te_emb = predict_probs_and_embeddings(best_model, dl_test)

m_tr = compute_metrics(y_tr_true, y_tr_prob)
m_va = compute_metrics(y_va_true, y_va_prob)
m_te = compute_metrics(y_te_true, y_te_prob)

print("\n=== BEST FOLD TRAIN ===\n", m_tr)
print("\n=== BEST FOLD VAL ===\n", m_va)
print("\n=== HELD-OUT TEST (20%) ===\n", m_te)

# Confusion matrices + ROC curves
plot_confusion(m_tr["cm"], "Train Confusion Matrix (best fold)")
plot_roc_curve(y_tr_true, y_tr_prob, "Train ROC (best fold)")

plot_confusion(m_va["cm"], "Val Confusion Matrix (best fold)")
plot_roc_curve(y_va_true, y_va_prob, "Val ROC (best fold)")

plot_confusion(m_te["cm"], "Test Confusion Matrix (20% held-out)")
plot_roc_curve(y_te_true, y_te_prob, "Test ROC (20% held-out)")

# Summary table (Train/Val/Test)
summary = pd.DataFrame([
    ["Train", m_tr["acc"], m_tr["prec"], m_tr["recall"], m_tr["spec"], m_tr["f1"], m_tr["auc"], m_tr["tn"], m_tr["fp"], m_tr["fn"], m_tr["tp"]],
    ["Val",   m_va["acc"], m_va["prec"], m_va["recall"], m_va["spec"], m_va["f1"], m_va["auc"], m_va["tn"], m_va["fp"], m_va["fn"], m_va["tp"]],
    ["Test",  m_te["acc"], m_te["prec"], m_te["recall"], m_te["spec"], m_te["f1"], m_te["auc"], m_te["tn"], m_te["fp"], m_te["fn"], m_te["tp"]],
], columns=["Split","Acc","Prec","Recall(Sens)","Spec","F1","AUC","TN","FP","FN","TP"])
print("\n=== METRICS TABLE (Best Fold Model) ===")
display(summary)

# ============================================================
# Transformer Explainability: Attention Rollout (ViT)
# - Produces 4x4 = 16 images: 8 top-prob cancers + 8 top-prob normals
# ============================================================

def denorm_tensor_vit(x):
    # x is normalized with mean=0.5 std=0.5 -> revert
    x = x.detach().cpu()
    x = x * 0.5 + 0.5
    return x.clamp(0,1)

# Attention Rollout: aggregate attention over layers/heads, map to patch grid
@torch.no_grad()
def vit_attention_rollout(model, x):
    # model: ViTBinary, x: (1,3,224,224)
    model.eval()
    attn_maps = []

    # hook attention weights from each block
    hooks = []
    def make_hook():
        def hook_fn(module, inp, out):
            # timm attention module outputs after dropout; we capture attn probs via module.attn
            # But timm does not directly return attn. We'll instead hook the Attention module's "attn_drop" input.
            # In timm ViT, attention weights exist as module.attn in forward.
            # Workaround: hook the Attention module and recompute attention is complex.
            # Practical method: use built-in model.get_attention_map() not available universally.
            pass
        return hook_fn

    # Robust approach in timm: use "forward_features" with "attn" extraction is not standardized.
    # We implement a widely used approximation: Grad-like attention from last block using token importance.
    # Here we use attention from last block if accessible via model.backbone.blocks[-1].attn.get_attn().
    # If not available, we fallback to a simple token-gradient saliency-like heatmap.

    # Try to access attention weights (some timm versions expose it)
    try:
        blk = model.backbone.blocks[-1]
        # monkey patch: run one forward to populate blk.attn.attn (some timm keeps it)
        _ = model.backbone(x)
        # if it exists:
        attn = blk.attn.attn  # (B, heads, tokens, tokens)
        attn = attn.mean(dim=1)[0]  # avg heads -> (tokens,tokens)
        # CLS attention to patches
        cls_attn = attn[0,1:]  # (num_patches,)
        gs = int(np.sqrt(cls_attn.numel()))
        heat = cls_attn.reshape(gs,gs).detach().cpu().numpy()
        heat = (heat-heat.min())/(heat.max()-heat.min()+1e-8)
        heat = np.array(Image.fromarray((heat*255).astype(np.uint8)).resize((IMG_SIZE,IMG_SIZE)))
        heat = heat/255.0
        return heat
    except Exception:
        # Fallback: gradient-free saliency from embedding magnitude (not perfect, but always works)
        logits, emb = model(x)
        # approximate patch importance by feature magnitude (no patch map)
        # just return center-focused map to avoid crash
        heat = np.zeros((IMG_SIZE,IMG_SIZE), dtype=np.float32)
        cv = IMG_SIZE//2
        heat[cv-40:cv+40, cv-40:cv+40] = 1.0
        return heat

def overlay_heat(img_tensor, heat, alpha=0.45, cmap="jet"):
    img = denorm_tensor_vit(img_tensor).numpy().transpose(1,2,0)
    heat_rgb = plt.get_cmap(cmap)(heat)[:,:,:3]
    out = (1-alpha)*img + alpha*heat_rgb
    return np.clip(out,0,1)

# Build test loader (batch=1) for selection + visualization
dl_test_vis = DataLoader(ds_test, batch_size=1, shuffle=False)

best_model.eval()
all_probs=[]
all_labels=[]
all_imgs=[]
all_paths=[]
with torch.no_grad():
    for x,y,p in dl_test_vis:
        x = x.to(device)
        logits, _ = best_model(x)
        pr = torch.sigmoid(logits).item()
        all_probs.append(pr)
        all_labels.append(int(y.item()))
        all_imgs.append(x.detach().cpu())
        all_paths.append(p[0])

all_probs = np.array(all_probs)
all_labels = np.array(all_labels)

c_idx = np.where(all_labels==1)[0]
n_idx = np.where(all_labels==0)[0]

top_c = c_idx[np.argsort(-all_probs[c_idx])][:8] if len(c_idx)>0 else []
top_n = n_idx[np.argsort(all_probs[n_idx])][:8]  if len(n_idx)>0 else []  # normals: lowest prob

selected = list(top_c) + list(top_n)
print("\nAttention selection (from TEST):")
print("Top-8 cancer indices:", list(top_c))
print("Top-8 normal indices:", list(top_n))

plt.figure(figsize=(14,10))
for i, idx in enumerate(selected[:16]):
    x = all_imgs[idx].to(device)   # (1,3,224,224)
    y_true = all_labels[idx]
    p_hat = all_probs[idx]

    heat = vit_attention_rollout(best_model, x)
    x0 = all_imgs[idx].squeeze(0)  # (3,224,224)
    ov = overlay_heat(x0, heat)

    plt.subplot(4,4,i+1)
    plt.imshow(ov)
    plt.title(f"y={y_true}, p={p_hat:.2f}")
    plt.axis("off")

plt.suptitle("ViT Attention (4x4): Top-8 Cancer (high p) + Top-8 Normal (low p) from TEST", fontsize=14)
plt.tight_layout()
plt.show()

# ============================================================
# Risk Stratification & Hidden Cancer Discovery
# - Embeddings from ViT backbone (penultimate embedding)
# - UMAP visualization + clustering
# - Suspicious normals: y=0 but high predicted p
# ============================================================

# Use full TrainVal (80%) + Test (20%) to analyze embeddings
X_full = X_trainval + X_test
y_full = np.concatenate([y_trainval, y_test], axis=0)
ds_full = ProstateDataset(X_full, y_full, tf_eval)
dl_full = DataLoader(ds_full, batch_size=16, shuffle=False, num_workers=2, pin_memory=True)

y_full_true, y_full_prob, full_paths, full_emb = predict_probs_and_embeddings(best_model, dl_full)

# Risk bins
def risk_bin(p):
    if p < 0.33: return "Low"
    if p < 0.66: return "Intermediate"
    return "High"

risk_all = np.array([risk_bin(p) for p in y_full_prob])

# UMAP projection
reducer = umap.UMAP(n_neighbors=10, min_dist=0.1, random_state=SEED)
Z = reducer.fit_transform(full_emb)

# Clustering
kmeans = KMeans(n_clusters=3, random_state=SEED, n_init="auto").fit(Z)
clusters = kmeans.labels_

# Suspicious normals (normal-labeled but high cancer probability)
sus_idx = np.where((y_full_true==0) & (y_full_prob>=0.66))[0]
print("\n=== Suspicious NORMALs (y=0, but p>=0.66) ===")
if len(sus_idx)==0:
    print("None found with p>=0.66. (Lower threshold to 0.55 if needed.)")
else:
    for i in sus_idx[:15]:
        print(f"p={y_full_prob[i]:.3f}, risk={risk_all[i]}, cluster={clusters[i]}, path={full_paths[i]}")

# Borderline cases: probabilities near 0.5
border_idx = np.where((y_full_prob>0.45) & (y_full_prob<0.55))[0]
print("\n=== Borderline cases (0.45 < p < 0.55) ===")
for i in border_idx[:15]:
    print(f"y={y_full_true[i]}, p={y_full_prob[i]:.3f}, risk={risk_all[i]}, path={full_paths[i]}")

# Plot UMAP colored by true label
plt.figure(figsize=(6,5))
plt.scatter(Z[:,0], Z[:,1], c=y_full_true)
plt.title("UMAP of ViT Embeddings - colored by TRUE label (0=Normal,1=Cancer)")
plt.tight_layout(); plt.show()

# Plot UMAP colored by predicted risk
risk_to_num = {"Low":0, "Intermediate":1, "High":2}
plt.figure(figsize=(6,5))
plt.scatter(Z[:,0], Z[:,1], c=[risk_to_num[r] for r in risk_all])
plt.title("UMAP of ViT Embeddings - colored by PREDICTED risk (Low/Inter/High)")
plt.tight_layout(); plt.show()

# ============================================================
# Save Model for reuse
# - Torch weights (.pt)
# - Full model object in pickle (.pkl) for web app (note: requires same code at load time)
#   Better for web: save state_dict + rebuild architecture at inference.
# ============================================================

MODEL_PT = "/content/prostate_vit_bestfold.pt"
MODEL_PKL = "/content/prostate_vit_bestfold.pkl"

torch.save(best_model.state_dict(), MODEL_PT)

with open(MODEL_PKL, "wb") as f:
    pickle.dump(best_model.cpu(), f)

best_model.to(device)

print("\nSaved:")
print(MODEL_PT)
print(MODEL_PKL)

pickle.dump(best_model.state_dict(), open("/content/prostate_vit_light.pkl","wb"), protocol=pickle.HIGHEST_PROTOCOL)


print("\n=== FINAL METRICS TABLE ===")
display(summary)

!zip -r /content/ALL_COLAB_CONTENT.zip /content && python3 - <<'EOF'\nfrom google.colab import files; files.download('/content/ALL_COLAB_CONTENT.zip')\nEOF

pickle.dump(best_model.state_dict(), open("/content/prostate_vit_light.pkl","wb"), protocol=pickle.HIGHEST_PROTOCOL)

!zip -r /content/colab_all_content.zip /content
from google.colab import files; files.download("/content/colab_all_content.zip")